{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# url = 'http://cirtec.ranepa.ru/Word2Vec/fixes.raw.txt'\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read()      # a `bytes` object\n",
    "\n",
    "# with open('../initial_data/Word2Vec__fixes.stem.txt', 'wb') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('../initial_data/Word2Vec__fixes.stem.txt', 'rb') as f:\n",
    "    data = f.read()\n",
    "text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "del data\n",
    "print(\"len_text: {}\".format(len(text)))\n",
    "sentences = [sentence for sentence in text.split(\"\\n\") if len(sentence) > 60]\n",
    "# # debug\n",
    "# sentences = sentences[:1000]\n",
    "del text\n",
    "print(\"len_sentences: {}\".format(len(sentences)))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from base_defs import get_average_len_words\n",
    "\n",
    "min_average_len_words = 8\n",
    "sentences = [[word for word in sentence.split()] for sentence in sentences\n",
    "             if get_average_len_words(sentence) >= min_average_len_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pymorphy2\n",
    "import gensim\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "#\n",
    "# http://rusvectores.org/ru/models/\n",
    "#     web_upos_cbow_300_20_2017\n",
    "#\n",
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('../models/web_0_300_20.bin', binary=True)\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "cotags = {\n",
    "    'ADJF' : 'DET',\n",
    "    'ADJS' : 'ADJ', \n",
    "    'ADVB' : 'ADV', \n",
    "    'COMP' : 'ADV', \n",
    "    'GRND' : 'VERB', \n",
    "    'INFN' : 'VERB', \n",
    "    'NOUN' : 'NOUN', \n",
    "    'PRED' : 'ADV', \n",
    "    'NPRO' : 'PRON', \n",
    "    'PREP' : 'ADV', \n",
    "    'CONJ' : 'Ð¡CONJ',\n",
    "    'PRTF' : 'ADJ', \n",
    "    'PRTS' : 'VERB', \n",
    "    'VERB' : 'VERB',\n",
    "    'INTJ' : 'INTJ',\n",
    "    'PRCL' : 'PART',\n",
    "    'NUMR' : 'NUM',\n",
    "    'None' : 'NOUN',\n",
    "}\n",
    "\n",
    "def get_word_with_correct_tag(word_and_tag):\n",
    "    sp = word_and_tag.split(\"_\")\n",
    "    if len(sp) != 2:\n",
    "        print(sp)\n",
    "    return \"_\".join((sp[0], cotags[str(sp[1])]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set correct tags, create vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vectors = [dict(vector=np.mean([w2v_model.get_vector(word) for word in words if word in w2v_model.vocab], axis=0), number=number) for number, words in enumerate([[get_word_with_correct_tag(word) for word in sentence.split()] for sentence in sentences])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = []\n",
    "\n",
    "df_vectors = pd.DataFrame(vectors)\n",
    "\n",
    "threshold = 0.15\n",
    "min_size_cluster = 100\n",
    "\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while df_vectors.shape[0] > 10000:\n",
    "    vector_0 = df_vectors.vector.iloc[0]\n",
    "    df_vectors['res'] = df_vectors.vector.apply(lambda vector: cosine(vector_0, vector))\n",
    "    cluster = df_vectors[df_vectors['res'] <= threshold]\n",
    "    df_vectors = df_vectors[df_vectors['res'] > threshold]\n",
    "    if cluster.shape[0] > min_size_cluster:\n",
    "        clusters.append(cluster)\n",
    "        counter += 1\n",
    "        print('shape of new cluster: {:4}, counter: {}, dataset:{}'.format(cluster.shape[0], counter, df_vectors.shape[0]))\n",
    "        with open(\"../resulting_data/clustering_analysis_stem__web_0_300_20.bin/{}.csv\".format(counter), \"w\") as f:\n",
    "            f.writelines((\"{}\\n\".format(sen) for sen in (\" \".join([word.split(\"_\")[0] for word in sentences[number].split()]) for number in cluster.number)))\n",
    "        break\n",
    "    else:\n",
    "        # print(\"drop: {}\".format(cluster.shape[0]))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
