{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# url = 'http://cirtec.ranepa.ru/analysis/Word2Vec/citcon4bundles.txt'\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read()      # a `bytes` object\n",
    "\n",
    "# with open('../initial_data/Word2Vec__citcon4bundles.txt', 'wb') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_len_words(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return sum([len(word) for word in words]) / len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('../initial_data/Word2Vec__citcon4bundles.txt', 'rb') as f:\n",
    "    data = f.read()\n",
    "text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "del data\n",
    "print(\"len_text: {}\".format(len(text)))\n",
    "sentences = [(sentence[0], sentence[1], sentence[2], sentence[3], sentence[4].replace(\"\\xad \", \"\").lower()) \n",
    "             for sentence in (sentence.split(\" \", 4) for sentence in text.split(\"\\n\") if sentence and get_average_len_words(sentence) > 5)]\n",
    "del text\n",
    "print(\"len_sentences: {}\".format(len(sentences)))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import string\n",
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "regex = re.compile('[^а-яА-Я]')\n",
    "\n",
    "words = [(sentence[0], [morph.parse(word)[0].normal_form for word in regex.sub(' ', sentence[4]).split()]) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "def get_n_gram(_words, n, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"), min_count=20, threshold=10):\n",
    "    if n < 2:\n",
    "        raise ValueError(\" n < 2 \")\n",
    "    if len(delimiters) < n - 1:\n",
    "        raise ValueError(\" len(delimiters) < n-1 \")\n",
    "    grams = []\n",
    "    for ind in range(n - 1):\n",
    "        gram = Phrases(_words, min_count=min_count, delimiter=delimiters[ind], threshold=threshold)\n",
    "        grams.append(gram)\n",
    "        if ind != n - 2:\n",
    "            _words = gram[_words]\n",
    "    return grams\n",
    "\n",
    "\n",
    "def drop_all_delimiters(text, delimiters=(\"@\", \"#\", \"$\", \"%\")):\n",
    "    for delimiter in delimiters:\n",
    "        text = text.replace(delimiter, \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_gram_vocab(grams, n, delimiters=(\"@\", \"#\", \"$\", \"%\")):\n",
    "    if n < 2:\n",
    "        raise ValueError(\" n < 2 \")\n",
    "    if len(delimiters) < n - 1:\n",
    "        raise ValueError(\" len(delimiters) < n-1 \")\n",
    "    delimiters = delimiters[:n - 1]\n",
    "    sorted_gram_vocab = sorted([(drop_all_delimiters(value, delimiters), count) for (value, count) in\n",
    "                                [(value.decode('utf8'), count) for value, count in dict(grams[n - 2].vocab).items()]\n",
    "                                if sum([dlm in value for dlm in delimiters]) == n - 1], key=lambda kv: kv[1],\n",
    "                               reverse=True)\n",
    "\n",
    "    return sorted_gram_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grams = get_n_gram(_words=[word[1] for word in words], n=4, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = grams[0][[word[1] for word in words]]\n",
    "t3 = grams[1][t2]\n",
    "t4 = grams[2][t3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrases(_words, n, delimiters=(\"@\", \"#\", \"$\", \"%\")):\n",
    "    phrases = [[word for word in sent] for sent in _words] # _words\n",
    "    for ind in range(n - 1):\n",
    "        phrases = [[word for word in sent if delimiters[ind] in word] for sent in phrases]\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "phrases_t3 = [value for value in zip([word[0] for word in words], get_phrases(t3, 3))]\n",
    "phrases_t4 = [value for value in zip([word[0] for word in words], get_phrases(t4, 4))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unique = list(set([word[0] for word in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_res_3 = dict()\n",
    "\n",
    "for un in unique:\n",
    "    all_ph = []\n",
    "    for i in (ph for ph in phrases_t3 if ph[0] == un):\n",
    "        all_ph.extend(i[1])\n",
    "    dict_res_3[un] = Counter(all_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_res_4 = dict()\n",
    "\n",
    "for un in unique:\n",
    "    all_ph = []\n",
    "    for i in (ph for ph in phrases_t4 if ph[0] == un):\n",
    "        all_ph.extend(i[1])\n",
    "    dict_res_4[un] = Counter(all_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_res_4[unique[10]].most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_3 = {un: [drop_all_delimiters(m_c[0]) for m_c in dict_res_3[un].most_common()] for un in unique}\n",
    "res_4 = {un: [drop_all_delimiters(m_c[0]) for m_c in dict_res_4[un].most_common()] for un in unique}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../resulting_data/json_grams/3_gram_normalize.json\", \"w\") as f:\n",
    "    json.dump({key: value if value else None for key, value in res_3.items()}, f)\n",
    "with open(\"../resulting_data/json_grams/4_gram_normalize.json\", \"w\") as f:\n",
    "    json.dump({key: value if value else None for key, value in res_4.items()}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
