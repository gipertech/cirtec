{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# url = 'http://cirtec.ranepa.ru/analysis/Word2Vec/citcon4bundles.txt'\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read()      # a `bytes` object\n",
    "\n",
    "# with open('../initial_data/Word2Vec__citcon4bundles.txt', 'wb') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка файл, подготовка данных в виде:\n",
    "1) массива слов\n",
    "2) массива нормализованных слов\n",
    "\n",
    "~10 минут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import string\n",
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "regex = re.compile('[^а-яА-Я]')\n",
    "\n",
    "def get_average_len_words(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return sum([len(word) for word in words]) / len(words)\n",
    "\n",
    "with open('../initial_data/Word2Vec__citcon4bundles.txt', 'rb') as f:\n",
    "    data = f.read()\n",
    "text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "del data\n",
    "sentences = [(sentence[0], sentence[1], sentence[2], sentence[3], sentence[4].replace(\"\\xad \", \"\").lower()) \n",
    "             for sentence in (sentence.split(\" \", 4) for sentence in text.split(\"\\n\") if sentence and get_average_len_words(sentence) > 5)]\n",
    "del text\n",
    "print(\"len_sentences: {}\".format(len(sentences)))\n",
    "\n",
    "words = [(sentence[0], [word for word in regex.sub(' ', sentence[4]).split()]) for sentence in sentences]\n",
    "words_normal_form = [(sentence[0], [morph.parse(word)[0].normal_form for word in regex.sub(' ', sentence[4]).split()]) for sentence in sentences]\n",
    "\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "def get_n_gram(_words, n, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"), min_count=20, threshold=50):\n",
    "    if n < 2:\n",
    "        raise ValueError(\" n < 2 \")\n",
    "    if len(delimiters) < n - 1:\n",
    "        raise ValueError(\" len(delimiters) < n-1 \")\n",
    "    grams = []\n",
    "    for ind in range(n - 1):\n",
    "        gram = Phrases(_words, min_count=min_count, delimiter=delimiters[ind], threshold=threshold)\n",
    "        grams.append(gram)\n",
    "        if ind != n - 2:\n",
    "            _words = gram[_words]\n",
    "    return grams\n",
    "\n",
    "\n",
    "def drop_all_delimiters(text, delimiters=(\"@\", \"#\", \"$\", \"%\")):\n",
    "    for delimiter in delimiters:\n",
    "        text = text.replace(delimiter, \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_gram_vocab(grams, n, delimiters=(\"@\", \"#\", \"$\", \"%\")):\n",
    "    if n < 2:\n",
    "        raise ValueError(\" n < 2 \")\n",
    "    if len(delimiters) < n - 1:\n",
    "        raise ValueError(\" len(delimiters) < n-1 \")\n",
    "    delimiters = delimiters[:n - 1]\n",
    "    sorted_gram_vocab = sorted([(drop_all_delimiters(value, delimiters), count) for (value, count) in\n",
    "                                [(value.decode('utf8'), count) for value, count in dict(grams[n - 2].vocab).items()]\n",
    "                                if sum([dlm in value for dlm in delimiters]) == n - 1], key=lambda kv: kv[1],\n",
    "                               reverse=True)\n",
    "\n",
    "    return sorted_gram_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучения фразеров\n",
    "\n",
    "~2 минуты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_phrases(_list_words, n, delimiters=(\"@\", \"#\", \"$\", \"%\")):\n",
    "    phrases = [[word for word in _words] for _words in _list_words] # _words\n",
    "    for ind in range(n - 1):\n",
    "        phrases = [[word for word in sent if delimiters[ind] in word] for sent in phrases]\n",
    "    return phrases\n",
    "\n",
    "grams = get_n_gram(_words=[word[1] for word in words], n=3, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"))\n",
    "grams_normal_form = get_n_gram(_words=[word[1] for word in words_normal_form], n=3, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"))\n",
    "\n",
    "t3 = grams[1][grams[0][[word[1] for word in words]]]\n",
    "t3_normal_form = grams_normal_form[1][grams_normal_form[0][[word[1] for word in words_normal_form]]]\n",
    "\n",
    "phrases_t3 = [value for value in zip([word[0] for word in words], get_phrases(t3, 3))]\n",
    "phrases_t3_normal_form = [value for value in zip([word[0] for word in words_normal_form], get_phrases(t3_normal_form, 3))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "bunshid = list(set([word[0] for word in words]))\n",
    "\n",
    "def get_dict_counters_by_bunshid(__phrases, __bunshid):\n",
    "    dict_res = dict()\n",
    "    for un in __bunshid:\n",
    "        all_ph = []\n",
    "        for i in (ph for ph in __phrases if ph[0] == un):\n",
    "            all_ph.extend(i[1])\n",
    "        dict_res[un] = Counter(all_ph)\n",
    "    return dict_res\n",
    "\n",
    "dict_res_t3 = get_dict_counters_by_bunshid(phrases_t3, bunshid)\n",
    "dict_res_t3_normal_form = get_dict_counters_by_bunshid(phrases_t3_normal_form, bunshid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = 14\n",
    "\n",
    "# print(bunshid[ind])\n",
    "# print(dict_res_t3[bunshid[ind]].most_common())\n",
    "# print()\n",
    "# print(dict_res_t3_normal_form[bunshid[ind]].most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create result dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = dict()\n",
    "for b in bunshid:\n",
    "    result_dict[b] = {\n",
    "        \"3orig\": [{\"w\": drop_all_delimiters(m_c[0]), \"n\": m_c[1]} for m_c in dict_res_t3[b].most_common()],\n",
    "        \"3norm\": [{\"w\": drop_all_delimiters(m_c[0]), \"n\": m_c[1]} for m_c in dict_res_t3_normal_form[b].most_common()],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_dict[bunshid[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../resulting_data/json_grams/3_grams_v2.json\", \"w\") as f:\n",
    "    json.dump(result_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
