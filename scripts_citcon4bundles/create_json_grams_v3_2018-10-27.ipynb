{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'http://cirtec.ranepa.ru/analysis/Word2Vec/citcon4bundles.txt'\n",
    "response = urllib.request.urlopen(url)\n",
    "data = response.read()      # a `bytes` object\n",
    "\n",
    "with open('../initial_data/Word2Vec__citcon4bundles.txt', 'wb') as f:\n",
    "    f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка файл, подготовка данных в виде:\n",
    "1) массива слов\n",
    "2) массива нормализованных слов\n",
    "\n",
    "~10 минут"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import string\n",
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "regex = re.compile('[^а-яА-Я]')\n",
    "\n",
    "def get_average_len_words(sentence):\n",
    "    words = sentence.split()\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return sum([len(word) for word in words]) / len(words)\n",
    "\n",
    "with open('../initial_data/Word2Vec__citcon4bundles.txt', 'rb') as f:\n",
    "    data = f.read()\n",
    "text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "del data\n",
    "sentences = [(sentence[0], sentence[1], sentence[2], sentence[3], sentence[4].replace(\"\\xad \", \"\").lower()) \n",
    "             for sentence in (sentence.split(\" \", 4) for sentence in text.split(\"\\n\") if sentence and get_average_len_words(sentence) > 5)]\n",
    "del text\n",
    "print(\"len_sentences: {}\".format(len(sentences)))\n",
    "\n",
    "words = [(sentence[0], [word for word in regex.sub(' ', sentence[4]).split()]) for sentence in sentences]\n",
    "words_normal_form = [(sentence[0], [morph.parse(word)[0].normal_form for word in regex.sub(' ', sentence[4]).split()]) for sentence in sentences]\n",
    "\n",
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [(pair[0], [word for word in pair[1] if word not in [\"й\", \"ч\", \"р\"]]) for pair in words]\n",
    "\n",
    "words_normal_form = [(pair[0], [word for word in pair[1] if word not in [\"й\", \"ч\", \"р\"]]) for pair in words_normal_form]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "def get_n_gram(_words, n, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\", b\"^\", b\"&\", b\"*\"), min_count=20, threshold=50):\n",
    "    if n < 2:\n",
    "        raise ValueError(\" n < 2 \")\n",
    "    if len(delimiters) < n - 1:\n",
    "        raise ValueError(\" len(delimiters) < n-1 \")\n",
    "    grams = []\n",
    "    for ind in range(n - 1):\n",
    "        gram = Phrases(_words, min_count=min_count, delimiter=delimiters[ind], threshold=threshold)\n",
    "        grams.append(gram)\n",
    "        if ind != n - 2:\n",
    "            _words = gram[_words]\n",
    "    return grams\n",
    "\n",
    "\n",
    "def drop_all_delimiters(text, delimiters=(\"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\")):\n",
    "    for delimiter in delimiters:\n",
    "        text = text.replace(delimiter, \" \")\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_gram_vocab(grams, n, delimiters=(\"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\")):\n",
    "    if n < 2:\n",
    "        raise ValueError(\" n < 2 \")\n",
    "    if len(delimiters) < n - 1:\n",
    "        raise ValueError(\" len(delimiters) < n-1 \")\n",
    "    delimiters = delimiters[:n - 1]\n",
    "    sorted_gram_vocab = sorted([(drop_all_delimiters(value, delimiters), count) for (value, count) in\n",
    "                                [(value.decode('utf8'), count) for value, count in dict(grams[n - 2].vocab).items()]\n",
    "                                if sum([dlm in value for dlm in delimiters]) == n - 1], key=lambda kv: kv[1],\n",
    "                               reverse=True)\n",
    "\n",
    "    return sorted_gram_vocab\n",
    "\n",
    "\n",
    "def get_words_with_phrases(pairs, _list_grams):\n",
    "    _list_words = [pair[1] for pair in pairs]\n",
    "    for gram in _list_grams:\n",
    "        _list_words = gram[_list_words]\n",
    "    return list(zip([pair[0] for pair in pairs], _list_words))\n",
    "    \n",
    "\n",
    "def get_phrases(pairs, n, delimiters=(\"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\")):\n",
    "    _phrases = [pair[1] for pair in pairs]\n",
    "    for ind in range(n - 1):\n",
    "        _phrases = [[word for word in sent if delimiters[ind] in word] for sent in _phrases]\n",
    "\n",
    "    _phrases = [[word for word in sent if delimiters[n-1] not in word] for sent in _phrases]\n",
    "    return list(zip([pair[0] for pair in pairs], _phrases)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучения фразеров\n",
    "\n",
    "~2 минуты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "N = 6\n",
    "\n",
    "grams = get_n_gram(_words=[word[1] for word in words], n=N)\n",
    "grams_normal_form = get_n_gram(_words=[word[1] for word in words_normal_form], n=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "words_with_phrases = get_words_with_phrases(words, grams)\n",
    "words_normal_form_with_phrases = get_words_with_phrases(words_normal_form, grams_normal_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases_words = {i: get_phrases(words_with_phrases, i) for i in range(2, N+1)}\n",
    "phrases_words_normal_form = {i: get_phrases(words_normal_form_with_phrases, i) for i in range(2, N+1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "\n",
    "all_pairs_names = set([word[0] for word in words])\n",
    "\n",
    "def get_dict_counters_by_bunshid(pairs):\n",
    "    _res = dict()\n",
    "    for name_pair in all_pairs_names:\n",
    "        all_pair = []\n",
    "        for _words in (pair[1] for pair in pairs if pair[0] == name_pair):\n",
    "            all_pair.extend(_words)\n",
    "        _res[name_pair] = Counter(all_pair)\n",
    "    return _res\n",
    "\n",
    "dict_counters_words = {key: get_dict_counters_by_bunshid(pairs) for key, pairs in phrases_words.items()}\n",
    "dict_counters_words_normal_form = {key: get_dict_counters_by_bunshid(pairs) for key, pairs in phrases_words_normal_form.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create result dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_dict(__dict_counters, __pair_name, __n, __grams):\n",
    "    return [\n",
    "        {\n",
    "            \"w\": drop_all_delimiters(counter_words[0]),\n",
    "            \"n\": counter_words[1],\n",
    "            \"t\": __grams[__n-2].vocab[counter_words[0].encode(\"utf-8\")]\n",
    "        } for counter_words in __dict_counters[__n][__pair_name].most_common()\n",
    "    ]\n",
    "\n",
    "result_dict = {\n",
    "    pair_name: {\n",
    "        \"2orig\": get_res_dict(dict_counters_words, pair_name, 2, grams),\n",
    "        \"2norm\": get_res_dict(dict_counters_words_normal_form, pair_name, 2, grams_normal_form),\n",
    "        \n",
    "        \"3orig\": get_res_dict(dict_counters_words, pair_name, 3, grams),\n",
    "        \"3norm\": get_res_dict(dict_counters_words_normal_form, pair_name, 3, grams_normal_form),\n",
    "        \n",
    "        \"4orig\": get_res_dict(dict_counters_words, pair_name, 4, grams),\n",
    "        \"4norm\": get_res_dict(dict_counters_words_normal_form, pair_name, 4, grams_normal_form),\n",
    "        \n",
    "        \"5orig\": get_res_dict(dict_counters_words, pair_name, 5, grams),\n",
    "        \"5norm\": get_res_dict(dict_counters_words_normal_form, pair_name, 5, grams_normal_form),\n",
    "        \n",
    "        \"6orig\": get_res_dict(dict_counters_words, pair_name, 6, grams),\n",
    "        \"6norm\": get_res_dict(dict_counters_words_normal_form, pair_name, 6, grams_normal_form),\n",
    "    } for pair_name in all_pairs_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../resulting_data/json_grams/2-3-4-5-6_grams_v3.json\", \"w\") as f:\n",
    "    json.dump(result_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
