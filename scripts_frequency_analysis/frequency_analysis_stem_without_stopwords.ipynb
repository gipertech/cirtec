{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load(url), save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "# url = 'http://cirtec.ranepa.ru/Word2Vec/fixes.stem.txt'\n",
    "# response = urllib.request.urlopen(url)\n",
    "# data = response.read()      # a `bytes` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../initial_data/Word2Vec__fixes.stem.txt', 'wb') as f:\n",
    "#     f.write(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with open('../initial_data/Word2Vec__fixes.stem.txt', 'rb') as f:\n",
    "    data = f.read()\n",
    "text = data.decode('utf-8') # a `str`; this step can't be used if data is binary\n",
    "del data\n",
    "print(\"len_text: {}\".format(len(text)))\n",
    "sentences = [sentence.lower() for sentence in text.split(\"\\n\") if len(sentence) > 60]\n",
    "del text\n",
    "print(\"len_sentences: {}\".format(len(sentences)))\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "from base_defs import get_average_len_words, get_n_gram, drop_all_delimiters, get_gram_vocab\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "stop_words = [\"{}_{}\".format(word, morph.parse(word)[0].tag.POS).lower() for word in stopwords.words('russian')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from base_defs import get_average_len_words, get_n_gram, drop_all_delimiters, get_gram_vocab\n",
    "\n",
    "min_average_len_words = 9\n",
    "sentences = [[word for word in sentence.split() if word not in stop_words] for sentence in sentences\n",
    "             if get_average_len_words(sentence) >= min_average_len_words]\n",
    "print(len(sentences))\n",
    "print(sentences[0][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# # part\n",
    "# grams = get_n_gram(sentences=sentences[:10000], n=4, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"))\n",
    "\n",
    "# all\n",
    "grams = get_n_gram(sentences=sentences, n=4, delimiters=(b\"@\", b\"#\", b\"$\", b\"%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "grams_vocab = []\n",
    "for ind in range(2, 5):\n",
    "     grams_vocab.extend(get_gram_vocab(grams=grams, n=ind, delimiters=(\"@\", \"#\", \"$\", \"%\")))\n",
    "res = pd.DataFrame(grams_vocab, columns=[\"text\", \"freq\"]).drop_duplicates([\"text\"]).sort_values([\"freq\"], ascending=False).reset_index(drop=True)\n",
    "res[\"len_text\"] = res.text.apply(lambda x: len(x.split()))\n",
    "res['text_without_tags'] = res['text'].apply(lambda x: \" \".join([word.split(\"_\")[0] for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_csv(res_df, path, len_text, head_size=3000):\n",
    "    save = res_df[res_df.len_text == len_text][[\"text_without_tags\", \"freq\"]]\n",
    "    size = min((head_size, save.shape[0]))\n",
    "    print(\"All len: {}, save: {}\".format(save.shape[0], size))\n",
    "    \n",
    "    filename = \"{}/frequency_analysis_stem_without_stopwords_{}__{}.csv\".format(path, len_text, size)\n",
    "    print(filename)\n",
    "    save.head(head_size).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(res, path=\"../resulting_data\", len_text=2)\n",
    "save_csv(res, path=\"../resulting_data\", len_text=3)\n",
    "save_csv(res, path=\"../resulting_data\", len_text=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
